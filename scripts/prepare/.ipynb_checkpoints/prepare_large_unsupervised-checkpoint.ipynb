{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "with open('../../hyperparams.yml') as f:\n",
    "    configs=yaml.safe_load(f)\n",
    "    \n",
    "with open('../../data/dataset_config.yaml') as f:\n",
    "    dataset_configs=yaml.safe_load(f)\n",
    "\n",
    "data_dir=configs['data_dir']\n",
    "raw_files_dir='raw/clusters/'\n",
    "max_length=dataset_configs['sequence_length']\n",
    "aa_vocabulary=dataset_configs['aa_vocabulary']\n",
    "\n",
    "\n",
    "destination_dir='dataset/unsupervised_large_clusters/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "hdf5_file = h5py.File(data_dir+destination_dir+\"dataset.hdf5\", \"w\")\n",
    "sequences_encoded = hdf5_file.create_dataset(\"sequences\", (5000000, 512), chunks=(512, 512), dtype='i8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing shard_92.csv\n",
      "4362\n",
      "\n",
      "Processing shard_71.csv\n",
      "4588\n",
      "\n",
      "Processing shard_15.csv\n",
      "4289\n",
      "\n",
      "Processing shard_59.csv\n",
      "4589\n",
      "\n",
      "Processing shard_30.csv\n",
      "4634\n",
      "\n",
      "Processing shard_75.csv\n",
      "4610\n",
      "\n",
      "Processing shard_54.csv\n",
      "4504\n",
      "\n",
      "Processing shard_67.csv\n",
      "4519\n",
      "\n",
      "Processing shard_79.csv\n",
      "3913\n",
      "\n",
      "Processing shard_99.csv\n",
      "4606\n",
      "\n",
      "Processing shard_51.csv\n",
      "4434\n",
      "\n",
      "Processing shard_38.csv\n",
      "4820\n",
      "\n",
      "Processing shard_53.csv\n",
      "3809\n",
      "\n",
      "Processing shard_98.csv\n",
      "4805\n",
      "\n",
      "Processing shard_42.csv\n",
      "4833\n",
      "\n",
      "Processing shard_60.csv\n",
      "4469\n",
      "\n",
      "Processing shard_64.csv\n",
      "4660\n",
      "\n",
      "Processing shard_76.csv\n",
      "4636\n",
      "\n",
      "Processing shard_39.csv\n",
      "4580\n",
      "\n",
      "Processing shard_41.csv\n",
      "4544\n",
      "\n",
      "Processing shard_65.csv\n",
      "4361\n",
      "\n",
      "Processing shard_19.csv\n",
      "7775\n",
      "\n",
      "Processing shard_34.csv\n",
      "3014\n",
      "\n",
      "Processing shard_17.csv\n",
      "4526\n",
      "\n",
      "Processing shard_106.csv\n",
      "4667\n",
      "\n",
      "Processing shard_102.csv\n",
      "4392\n",
      "\n",
      "Processing shard_112.csv\n",
      "4648\n",
      "\n",
      "Processing shard_8.csv\n",
      "4672\n",
      "\n",
      "Processing shard_24.csv\n",
      "3911\n",
      "\n",
      "Processing shard_40.csv\n",
      "4664\n",
      "\n",
      "Processing shard_47.csv\n",
      "4286\n",
      "\n",
      "Processing shard_27.csv\n",
      "3970\n",
      "\n",
      "Processing shard_83.csv\n",
      "4241\n",
      "\n",
      "Processing shard_2.csv\n",
      "4618\n",
      "\n",
      "Processing shard_85.csv\n",
      "4215\n",
      "\n",
      "Processing shard_61.csv\n",
      "4395\n",
      "\n",
      "Processing shard_9.csv\n",
      "4693\n",
      "\n",
      "Processing shard_28.csv\n",
      "4047\n",
      "\n",
      "Processing shard_29.csv\n",
      "4658\n",
      "\n",
      "Processing shard_62.csv\n",
      "4332\n",
      "\n",
      "Processing shard_94.csv\n",
      "4663\n",
      "\n",
      "Processing shard_57.csv\n",
      "4398\n",
      "\n",
      "Processing shard_37.csv\n",
      "4513\n",
      "\n",
      "Processing shard_1.csv\n",
      "4423\n",
      "\n",
      "Processing shard_80.csv\n",
      "4134\n",
      "\n",
      "Processing shard_16.csv\n",
      "4720\n",
      "\n",
      "Processing shard_82.csv\n",
      "4205\n",
      "\n",
      "Processing shard_21.csv\n",
      "4689\n",
      "\n",
      "Processing shard_107.csv\n",
      "4209\n",
      "\n",
      "Processing shard_45.csv\n",
      "4785\n",
      "\n",
      "Processing shard_5.csv\n",
      "4762\n",
      "\n",
      "Processing shard_33.csv\n",
      "4761\n",
      "\n",
      "Processing shard_36.csv\n",
      "4518\n",
      "\n",
      "Processing shard_12.csv\n",
      "4762\n",
      "\n",
      "Processing shard_69.csv\n",
      "3877\n",
      "\n",
      "Processing shard_81.csv\n",
      "3980\n",
      "\n",
      "Processing shard_35.csv\n",
      "3935\n",
      "\n",
      "Processing shard_78.csv\n",
      "3918\n",
      "\n",
      "Processing shard_74.csv\n",
      "4612\n",
      "\n",
      "Processing shard_105.csv\n",
      "4220\n",
      "\n",
      "Processing shard_90.csv\n",
      "4226\n",
      "\n",
      "Processing shard_26.csv\n",
      "4394\n",
      "\n",
      "Processing shard_14.csv\n",
      "4562\n",
      "\n",
      "Processing shard_87.csv\n",
      "4459\n",
      "\n",
      "Processing shard_113.csv\n",
      "4644\n",
      "\n",
      "Processing shard_58.csv\n",
      "4821\n",
      "\n",
      "Processing shard_32.csv\n",
      "4520\n",
      "\n",
      "Processing shard_10.csv\n",
      "4737\n",
      "\n",
      "Processing shard_22.csv\n",
      "4634\n",
      "\n",
      "Processing shard_68.csv\n",
      "3960\n",
      "\n",
      "Processing shard_97.csv\n",
      "4580\n",
      "\n",
      "Processing shard_4.csv\n",
      "4581\n",
      "\n",
      "Processing shard_84.csv\n",
      "4161\n",
      "\n",
      "Processing shard_63.csv\n",
      "4496\n",
      "\n",
      "Processing shard_3.csv\n",
      "4632\n",
      "\n",
      "Processing shard_13.csv\n",
      "4648\n",
      "\n",
      "Processing shard_55.csv\n",
      "4501\n",
      "\n",
      "Processing shard_89.csv\n",
      "4210\n",
      "\n",
      "Processing shard_91.csv\n",
      "4341\n",
      "\n",
      "Processing shard_110.csv\n",
      "4186\n",
      "\n",
      "Processing shard_72.csv\n",
      "4170\n",
      "\n",
      "Processing shard_7.csv\n",
      "4715\n",
      "\n",
      "Processing shard_104.csv\n",
      "4378\n",
      "\n",
      "Processing shard_50.csv\n",
      "4233\n",
      "\n",
      "Processing shard_20.csv\n",
      "4686\n",
      "\n",
      "Processing shard_77.csv\n",
      "4041\n",
      "\n",
      "Processing shard_66.csv\n",
      "4575\n",
      "\n",
      "Processing shard_56.csv\n",
      "3566\n",
      "\n",
      "Processing shard_73.csv\n",
      "3931\n",
      "\n",
      "Processing shard_88.csv\n",
      "4285\n",
      "\n",
      "Processing shard_52.csv\n",
      "4793\n",
      "\n",
      "Processing shard_86.csv\n",
      "4751\n",
      "\n",
      "Processing shard_23.csv\n",
      "3911\n",
      "\n",
      "Processing shard_0.csv\n",
      "4367\n",
      "\n",
      "Processing shard_95.csv\n",
      "4660\n",
      "\n",
      "Processing shard_25.csv\n",
      "4730\n",
      "\n",
      "Processing shard_48.csv\n",
      "3487\n",
      "\n",
      "Processing shard_109.csv\n",
      "4012\n",
      "\n",
      "Processing shard_108.csv\n",
      "4082\n",
      "\n",
      "Processing shard_49.csv\n",
      "4034\n",
      "\n",
      "Processing shard_11.csv\n",
      "4721\n",
      "\n",
      "Processing shard_70.csv\n",
      "3889\n",
      "\n",
      "Processing shard_18.csv\n",
      "4379\n",
      "\n",
      "Processing shard_31.csv\n",
      "4391\n",
      "\n",
      "Processing shard_96.csv\n",
      "4041\n",
      "\n",
      "Processing shard_46.csv\n",
      "4690\n",
      "\n",
      "Processing shard_101.csv\n",
      "4569\n",
      "\n",
      "Processing shard_111.csv\n",
      "4705\n",
      "\n",
      "Processing shard_43.csv\n",
      "4813\n",
      "\n",
      "Processing shard_103.csv\n",
      "3353\n",
      "\n",
      "Processing shard_93.csv\n",
      "4502\n",
      "\n",
      "Processing shard_100.csv\n",
      "4662\n",
      "\n",
      "Processing shard_44.csv\n",
      "5000\n",
      "\n",
      "Processing shard_6.csv\n",
      "4815\n",
      "Total number of records: 505603\n",
      "Total number of cluster references: 505603\n",
      "Total number of sequence references: 505603\n"
     ]
    }
   ],
   "source": [
    "clusters_refs=pd.DataFrame()\n",
    "sequences_ids=pd.DataFrame()\n",
    "\n",
    "pointer=0\n",
    "for filename in os.listdir(data_dir+raw_files_dir):\n",
    "    print('\\nProcessing', filename)\n",
    "    \n",
    "    data=pd.read_csv(data_dir+raw_files_dir+filename)\n",
    "    \n",
    "    #compute length and remove too long seqs\n",
    "    data=data[data['sequence'].str.len()<(max_length-2)]\n",
    "    \n",
    "    data=data.groupby('cluster_ref').first().reset_index()\n",
    "\n",
    "    batch_size=len(data)\n",
    "    print(batch_size)\n",
    "    \n",
    "    clusters_refs=pd.concat([clusters_refs, data['cluster_ref']], ignore_index=True)\n",
    "    sequences_ids=pd.concat([sequences_ids, data['entry_id']], ignore_index=True)\n",
    "    \n",
    "    \n",
    "    batch_encoded=np.zeros((len(data), max_length), dtype=np.int8)\n",
    "    \n",
    "    seq_idx=0\n",
    "    for row in data.itertuples():\n",
    "        aa_seq=row.sequence\n",
    "        \n",
    "        seq_encoded=np.zeros((max_length), dtype=np.int8)\n",
    "        \n",
    "        seq_encoded[0]=aa_vocabulary['<BOS>']\n",
    "\n",
    "        for aa_idx, aa in enumerate(aa_seq):\n",
    "            if aa not in aa_vocabulary:\n",
    "                aa_token=aa_vocabulary['X']\n",
    "            else:\n",
    "                aa_token=aa_vocabulary[aa]\n",
    "                \n",
    "            seq_encoded[aa_idx+1]=aa_token\n",
    "        \n",
    "        seq_encoded[aa_idx+2]=aa_vocabulary['<EOS>']\n",
    "        \n",
    "        batch_encoded[seq_idx]=seq_encoded\n",
    "        seq_idx += 1\n",
    "        \n",
    "    sequences_encoded[pointer:pointer+batch_size]=batch_encoded\n",
    "\n",
    "    pointer+=batch_size\n",
    "\n",
    "print('Total number of records:', pointer)\n",
    "print('Total number of cluster references:', len(clusters_refs))\n",
    "print('Total number of sequence references:', len(sequences_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_refs=pd.DataFrame()\n",
    "sequences_ids=pd.DataFrame()\n",
    "\n",
    "pointer=0\n",
    "for filename in os.listdir(data_dir+raw_files_dir):\n",
    "    print('\\nProcessing', filename)\n",
    "    \n",
    "    data=pd.read_csv(data_dir+raw_files_dir+filename)\n",
    "    \n",
    "    #compute length and remove too long seqs\n",
    "    data=data[data['sequence'].str.len()<(max_length-2)]\n",
    "    \n",
    "    batch_size=len(data)\n",
    "    print(batch_size)\n",
    "    \n",
    "    clusters_refs=pd.concat([clusters_refs, data['cluster_ref']], ignore_index=True)\n",
    "    sequences_ids=pd.concat([sequences_ids, data['entry_id']], ignore_index=True)\n",
    "    \n",
    "    \n",
    "    batch_encoded=np.zeros((len(data), max_length), dtype=np.int8)\n",
    "    \n",
    "    seq_idx=0\n",
    "    for row in data.itertuples():\n",
    "        aa_seq=row.sequence\n",
    "        \n",
    "        seq_encoded=np.zeros((max_length), dtype=np.int8)\n",
    "        \n",
    "        seq_encoded[0]=aa_vocabulary['<BOS>']\n",
    "\n",
    "        for aa_idx, aa in enumerate(aa_seq):\n",
    "            if aa not in aa_vocabulary:\n",
    "                aa_token=aa_vocabulary['X']\n",
    "            else:\n",
    "                aa_token=aa_vocabulary[aa]\n",
    "                \n",
    "            seq_encoded[aa_idx+1]=aa_token\n",
    "        \n",
    "        seq_encoded[aa_idx+2]=aa_vocabulary['<EOS>']\n",
    "        \n",
    "        batch_encoded[seq_idx]=seq_encoded\n",
    "        seq_idx += 1\n",
    "        \n",
    "    sequences_encoded[pointer:pointer+batch_size]=batch_encoded\n",
    "\n",
    "    pointer+=batch_size\n",
    "\n",
    "print('Total number of records:', pointer)\n",
    "print('Total number of cluster references:', len(clusters_refs))\n",
    "print('Total number of sequence references:', len(sequences_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(505603, 512)\n"
     ]
    }
   ],
   "source": [
    "sequences_encoded.resize((pointer, max_length))\n",
    "print(sequences_encoded.shape)\n",
    "hdf5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_refs.columns=['cluster_ref']\n",
    "clusters_refs.to_csv(data_dir+destination_dir+'clusters_refs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_ids.columns=['sequence_id']\n",
    "sequences_ids.to_csv(data_dir+destination_dir+'sequences_ids.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split in training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=h5py.File(data_dir+destination_dir+\"dataset.hdf5\")['sequences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs=np.arange(dataset.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 404483 Test dataset: 101120\n"
     ]
    }
   ],
   "source": [
    "test_ratio=0.2\n",
    "test_size=int(pointer*test_ratio)\n",
    "print('Train dataset:', pointer-test_size, 'Test dataset:', test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_file=h5py.File(data_dir+destination_dir+\"validation_dataset.hdf5\", \"w\")\n",
    "validation_dataset=val_file.create_dataset(\"sequences\", (5000000, 512), chunks=(512,512), dtype='i8')\n",
    "\n",
    "train_file=h5py.File(data_dir+destination_dir+\"train_dataset.hdf5\", \"w\")\n",
    "train_dataset=train_file.create_dataset(\"sequences\", (5000000, 512), chunks=(512,512), dtype='i8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101556\n",
      "404047\n"
     ]
    }
   ],
   "source": [
    "train_pointer=0\n",
    "test_pointer=0\n",
    "for b_start in range(0, len(idxs), 4096):\n",
    "    b_end=b_start+4096\n",
    "    \n",
    "    #dataset batch\n",
    "    dataset_batch=dataset[b_start:b_end]\n",
    "    \n",
    "    batch_idxs=idxs[b_start:b_end]\n",
    "    np.random.shuffle(batch_idxs)\n",
    "    batch_idxs -= np.min(batch_idxs)\n",
    "    test_idxs=np.sort(batch_idxs[:int(4096*test_ratio)]).tolist()\n",
    "    train_idxs=np.sort(batch_idxs[int(4096*test_ratio):]).tolist()\n",
    "    \n",
    "    shifter_test=len(test_idxs)\n",
    "    shifter_train=len(train_idxs)\n",
    "    \n",
    "    validation_dataset[test_pointer:test_pointer+shifter_test]=dataset_batch[test_idxs]\n",
    "    train_dataset[train_pointer:train_pointer+shifter_train]=dataset_batch[train_idxs]\n",
    "    \n",
    "    test_pointer+=shifter_test\n",
    "    train_pointer+=shifter_train\n",
    "    \n",
    "print(test_pointer)\n",
    "print(train_pointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset.resize((test_pointer, max_length))\n",
    "train_dataset.resize((train_pointer, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101556, 512)\n",
      "(404047, 512)\n"
     ]
    }
   ],
   "source": [
    "print(validation_dataset.shape)\n",
    "print(train_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_file.close()\n",
    "train_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
